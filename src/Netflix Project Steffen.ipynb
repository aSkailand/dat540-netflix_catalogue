{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "https://www.kaggle.com/niharika41298/netflix-visualizations-recommendation-eda\n",
    "https://jovian.ai/dwiknrd/eda-netflix/v/32?utm_source=embed\n",
    "\n",
    "\n",
    "* Frequent genre, actor, director\n",
    "* Amount of movies vs Genre\n",
    "* Year wise analysis, both movies & tv-shows\n",
    "* Show what country\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.countplot(x=\"type\", data=netflix_data, palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(y=\"release_year\", data=netflix_data, \n",
    "                   palette=\"Set2\", order=netflix_data.release_year.value_counts().index[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order =  sorted(netflix_data.release_year.unique())[-15:-1]\n",
    "plt.figure(figsize=(15,7))\n",
    "g = sns.countplot(netflix_data.release_year, hue=netflix_data.type, order=order, palette=\"pastel\");\n",
    "plt.title(\"Movies vs TV-Shows released on Netflix\")\n",
    "plt.xlabel(\"Production year\")\n",
    "plt.ylabel(\"Total Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the rate of Movies & TV Shows released on Netflix. Suddenly in 2019 we can see a clear change in the pattern. There can be several reasons. \n",
    "Disney + were launched in 2019, taking away movies that were previously on Netflix. We can also see a jump in TV series in 2018 as previously seen in 2016 for Movies, this can be a reason why Netflix is jumping on the Series train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating vs Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"rating\", data=netflix_data, palette=\"Set2\", order=netflix_data['rating'].value_counts().index[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_order =  ['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']\n",
    "\n",
    "movie_rating = netflix_data['rating'].value_counts()\n",
    "#tv_rating = tv_show['rating'].value_counts()[rating_order].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "        \"& Talk\": \"\",\n",
    "        \"Classic & Cult\": \"Classic, Cult\",\n",
    "        \"Features\": \"\",\n",
    "        \"Series\": \"\",\n",
    "        \"Comedy\": \"Comedies\",\n",
    "        \"British\": \"International\",\n",
    "        \"Spanish-Language\": \"International\",\n",
    "        \"Children & Family\": \"Kids'\",\n",
    "        \"TV Shows\": \"\",\n",
    "        \"Movies\": \"\",\n",
    "        \"Docuseries\": \"Documentaries\",\n",
    "        \"& Talk Shows\": \"\",\n",
    "        \"Stand-Up\": \"\",\n",
    "        \"TV\": \"\",\n",
    "        \"Shows\": \"\",\n",
    "        \" \": \"\",\n",
    "    }\n",
    "\n",
    "temp2 = netflix_data.copy()\n",
    "temp2.listed_in = temp2.listed_in.replace(replacements, regex=True).str.split(',').apply(lambda x: [i for i in x if i != 'International'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = temp2.explode('listed_in')\n",
    "temp3 = temp3.rename(columns={'listed_in': 'Genre', 'rating': 'PG-Rating'})\n",
    "df = temp3.groupby(['Genre','PG-Rating']).size().unstack(fill_value=0)\n",
    "df = df[df > 0].fillna(0)\n",
    "\n",
    "df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[1:]).set_title(\"Genre vs PG Rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Ex 4. Try to predict the genre of a movie/show based on the director, actors, etc. using\n",
    "machine/deep learning techniques.\n",
    "\n",
    "Here we take a dive into a Neural Network model. It was decided to use the model [MLPClassifier][MLPClassifier] from the sklearn library. We will start this chapter with some displamers. \n",
    "\n",
    "Only the finalized model will be displayed here. The data has been polished the python file _cleaning_data_for_NN.py_ to improve the final product. The changes has been mainly to combine similar genres and to remove names of actor/actresses that were only displayed once in the dataset. We believed this would improve our model. \n",
    "\n",
    "As stated, the model has been ran several times, however, only the final model has been displayed below.Furthermore, the previous model has been saved and their report is located below in the [result](#Results) section.\n",
    "\n",
    "\n",
    "## Programming Logic\n",
    "\n",
    "The core of the programming logic for the final model is simple. In the _cleaning_data_for_NN.py_ file we start by cleaning the provided [netflix data][netflixData] set. Removing nan values and values that were incoherent. Movies/TV Shows labled _just_ Movies/TV Shows were dropped from the dataset, and genres labeled with \"TV Show\" or \"Movie\", as for example Romantic Movies ( or Romantic TV Shows) were renamed to just Romantic. The main reason were try to improve the amount of datapoints with the same genre and reducing the amount of classes in the output data. \n",
    "\n",
    "Afterwards the data were checked for duplicates. We had issues with some lables because they were mentioned twice in a row. The biggest issue were about \"International Movies/TV Shows\". Our decision to combine all International subject to a larger genre called \"International\" made it often appeared more than once in several row it was a subject of. We decided to remove duplicates and the Pandas Dataframe were saved to another CSV file, called _cleaned_Netflix_for_NN.csv_.\n",
    "\n",
    "[MLPClassifier]: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "[netflixData]: https://www.kaggle.com/shivamb/netflix-shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading of CSV file, creating and transforming X & y data.\n",
    "\n",
    "The file _final_NN_script.py_ contains the logic of the final Neural Network algorithm. It will be presented in parts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data=pd.read_csv(\"cleaned_Netflix_for_NN.csv\")\n",
    "\n",
    "smaller_data = netflix_data.copy()\n",
    "\n",
    "y = smaller_data.listed_in\n",
    "X = [','.join((d, c, r, t)) for d,c,r,t in zip(\n",
    "                                                smaller_data.director, \n",
    "                                                smaller_data.cast, \n",
    "                                                smaller_data.rating, \n",
    "                                                smaller_data.title\n",
    "                                            )]\n",
    "\n",
    "# Custom stop words for the CountVectorizer to ignore while transforming.\n",
    "customStopWords=['no cast', 'no director', 'movies', 'tv shows',\n",
    "                'lgbtq movies', 'teen tv shows', 'cult'] \n",
    "\n",
    "# Find all actors that only appears once in the dataset\n",
    "customCastStopWords = smaller_data.cast.str.split(', ').explode().value_counts()[\n",
    "    smaller_data.cast.str.split(', ').explode().value_counts() < 2].keys()\n",
    "customCastStopWords = [x.lower() for x in customCastStopWords] # Make all values lowercase\n",
    "\n",
    "# Add stopwords for vectorizer into single frozenset\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(customStopWords)\n",
    "stop_words = stop_words.union(list(customCastStopWords))\n",
    "\n",
    "# Split data into train and test data, training data = 80% of original data & test = 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1000) \n",
    "\n",
    "# Define the vectorizer algorithm\n",
    "matrix = CountVectorizer(\n",
    "    tokenizer=lambda row: [x.strip() for x in row.split(',') if x != ''], \n",
    "    stop_words=stop_words)\n",
    "\n",
    "# Transform X data\n",
    "x_train_fit = matrix.fit_transform(X_train)\n",
    "x_test_fit = matrix.transform(X_test)\n",
    "\n",
    "# Transform y data\n",
    "y_train_fit = matrix.fit_transform(y_train)\n",
    "y_test_fit = matrix.transform(y_test)\n",
    "\n",
    "# Printing out all genres in the y data\n",
    "print(\"Genres:\")\n",
    "for i in matrix.get_feature_names():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X & y data\n",
    "\n",
    "A few lines needs to be explained in the code section above. The fist is the making of the Y and X data (out and in data) used by the Neural Network algorithm. \n",
    "```python\n",
    "y = smaller_data.listed_in\n",
    "X = [','.join((d, c, r, t)) for d,c,r,t in zip(\n",
    "                                                smaller_data.director, \n",
    "                                                smaller_data.cast, \n",
    "                                                smaller_data.rating, \n",
    "                                                smaller_data.title\n",
    "                                            )]\n",
    "```\n",
    "\n",
    "y data is the output prediction, and in this model we are trying to predict genres from numerous columns of the data set. Alittle selfexplanatory that the y (out) data points needs to be the genres, which, in the dataset is called _listed_in_\n",
    "\n",
    "The X data has a simple ( and potentionally improved ) python _join_ method implementation. We make use of the _CountVectorizer_ method deployed by the sklearn library. This will be further explained later, now we just need to know the input of this can be either a string or byte. \n",
    "\n",
    "[From the countVectorizer documentation][countVectorizer]: \n",
    "> input : string {‘filename’, ‘file’, ‘content’}, default=’content’ <br>\n",
    "> _Otherwise the input is expected to be a sequence of items that can be of type string or byte._\n",
    "\n",
    "To get around this issue (easily) since we want several columns the join method were implemented. Here, for each row, we join all directors, cast members, the rating and the tile into single list of string. This is discussed in the [Improvment Chapter](#Improvments-and-further-research)\n",
    "\n",
    "[countVectorizer]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "#### Stopwords\n",
    "\n",
    "The next section explained is the stopword logic. Because the netflix data set has empty cells from nearly each column, we made use of the imputation method explained earlier and replaces the empty words with filler words as _No Director_ or _No Cast_. Because these words is selfmade and has no meaning, they needs to not be counted by the CountVectorizer method. Here the stopwords come into play. By writing _No Director_ or _No Cast_ as stopwords (among others), we can rely on the method to not take these words into account when making the fitted data.\n",
    "\n",
    "\n",
    "```python\n",
    "# Custom stop words for the CountVectorizer to ignore while transforming.\n",
    "customStopWords=['no cast', 'no director', 'movies', 'tv shows',\n",
    "                'lgbtq movies', 'teen tv shows', 'cult'] \n",
    "\n",
    "customCastStopWords = smaller_data.cast.str.split(', ').explode().value_counts()[\n",
    "    smaller_data.cast.str.split(', ').explode().value_counts() < 2].keys()\n",
    "                                                                                 \n",
    "customCastStopWords = [x.lower() for x in customCastStopWords] # Make all values lowercase\n",
    "\n",
    "# Add stopwords for vectorizer into single frozenset\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(customStopWords)\n",
    "stop_words = stop_words.union(list(customCastStopWords))\n",
    "```\n",
    "\n",
    "This section of code creates the stopwords for the final model. From previous test the ``` 'lgbtq movies', 'teen tv shows', 'cult' ``` values were added to the stopwords due to the results from the [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) from a previous model. \n",
    "Further it was decided to remove all actors that only appeared in one movie. The variable _customCastStopWords_ stores these values. \n",
    "\n",
    "#### CountVectorizer\n",
    "\n",
    "After splitting the data into both test and train datasets with a ratio of 20 to 80 percent of the original dataset, the CountVectorizer is used. The documentation explains the class from sklearn as this: \n",
    "> Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "The method is implemented with a lamdba function: ```  tokenizer=lambda row: [x.strip() for x in row.split(',') if x != ''] ``` The tokenizer paramter is explained as such: \n",
    "> tokenizer : callable, default=None <br>\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. \n",
    "\n",
    "This is the parameter that is used to split the words from each row. The lamdba function has been implemented to make use of the join method used to join all the different columns. Now each row gets split and stripped for whitespaces to make sure that every item is the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 1000 #round(x_train_fit.shape[1]*(2/3) + y_train_fit.shape[1])\n",
    "\n",
    "# Small datetime to check when ML started\n",
    "datetime_object = datetime.datetime.now()\n",
    "print(\"Begin ML: \", datetime_object)\n",
    "\n",
    "# Neural Network algorithm\n",
    "clf = MLPClassifier(hidden_layer_sizes=(hidden_layer,hidden_layer ),\n",
    "                    solver='adam', verbose=True, \n",
    "                    random_state=1, max_iter=50) \n",
    "\n",
    "clf.fit(x_train_fit, y_train_fit)\n",
    "\n",
    "# Small datetime to check when ML stopped\n",
    "datetime_object = datetime.datetime.now()\n",
    "print(\"End ML: \", datetime_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As this defined as a unbalanced multilabeled classification, we can look at the micro average results to best see the results of the NN model.\n",
    "By having the _drama_ genre as an example: \n",
    "\n",
    "Precision is calculated: <br>\n",
    "drama correctly indentified divided by drama correctly identified plus other genres identified as drama\n",
    "\n",
    "Recall is calculated:\n",
    "Drama correctly identified by drama correctly identified plus drama identified as other genres. \n",
    "##### First model\n",
    "\n",
    "First we present the Classification Report of the First model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from first_NN_model import first_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = first_NN_model()\n",
    "\n",
    "fileName = 'first_unflitered_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cleaning up the data we can still see a good precision score, however the recall score is lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving genre data\n",
    "\n",
    "The first attempt to improve the results were to redefine genres. Previous report says several genres had a precision and recall score of 0, thus we made the decision to combine the genres and/or removed them from the transformed dataset. The resulting report is seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaned_genres_script import cleaned_genres_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = cleaned_genres_NN_model()\n",
    "\n",
    "fileName = 'cleaned_genres_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows a good improvment from 0.19 to 0.38 when we cleaned the genres up. We halved the amount of genres from 42  to 22. Our hypothesis to get more refined genres had a positiv result. However, we can still try to get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final result\n",
    "\n",
    "The final model removed all actors that only appeared in one movie, (approx 20k names). The logic is seen in the [first subchapter of Programming Logic](#Reading-of-CSV-file,-creating-and-transforming-X-&-y-data.) chapter. We assumed that these names would only confuse the model and not be helpful. The results are printed below in the already saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import values needed for classification report\n",
    "from final_NN_script import final_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = final_NN_model()\n",
    "# Compute results\n",
    "fileName = 'Final_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually see an further improvment on the recall and a small reduction in the precision. Can further small adjustments improve the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = multilabel_confusion_matrix(y_test_fit, y_pred)\n",
    "\n",
    "# Make labels out of genres\n",
    "labels = matrix.get_feature_names()\n",
    " \n",
    "def mutlilabel_cm_plot(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_xlabel('True label')\n",
    "    axes.set_ylabel('Predicted label')\n",
    "    axes.set_title(class_label)\n",
    "    \n",
    "# Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(6, 4, figsize=(7, 12))\n",
    "    \n",
    "for axes, cfs_matrix, label in zip(ax.flatten(), cm, labels):\n",
    "    mutlilabel_cm_plot(cfs_matrix, axes, label, [\"Y\", \"N\"])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end we show the confusion matrixes of all the genres from the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvments and further research\n",
    "\n",
    "Here arqe some features that could have been improved given more knowledge and time\n",
    "\n",
    "#### Questions\n",
    "* Could a word embedding model be used to further generalize the names of actor/actresses? \n",
    "* Could the title be used in a better extend? Maybe split and remove more of the same word and not use whole title string as an node?\n",
    "* Could word embedding be used to generelize the training/test data better than the manuall cleaning?\n",
    "\n",
    "#### Improvments\n",
    "* The model could be more refined to accept functions and/or make it more resuable.\n",
    "* The cleaning could be refined to accept different datasets, the genre that were changed is now hardcoded in a dictonary. This is not optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
