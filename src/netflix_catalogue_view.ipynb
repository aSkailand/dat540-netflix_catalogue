{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Netflix catalogue\n",
    "## Visualization, data mining and prediction of data\n",
    "This notebook contains the results by using the aformentioned techniques in order to get an understanding of how the netflix catalogue has evolved over the years as well as representing the data in an understandable manner.\n",
    "\n",
    "Most of the python code that does the heavy lifting i.e data sanitizing, crunching of numbers and magic resides in seperate .py files in order to keep the notbook clean, only featuring code that is relevant for plot and other visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python setup\n",
    "We recommend opening this notebook using JupyterLab in order to be able to view all the interactive plots.\n",
    "\n",
    "NB! Please only run this one time as the change director command will keep moving the notebook directory up the directory tree, this causes the filepath defined in the code to not agree with the current directory of the notebook. Restart kernel if IOError occurs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell one time. Restart kernel and run again to fix IOError.\n",
    "import os\n",
    "import yrs_months\n",
    "\n",
    "# Changes the notebook working directory on level up.\n",
    "%cd ..\n",
    "\n",
    "# Running main python script.\n",
    "%run -i \"src/main.py\"\n",
    "\n",
    "# Data set from main.py\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Genre analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Popular movie and series genres\n",
    "### Discarding the 'movie' and 'tv show'  entry in the genre list and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed for visualization\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import chord # Need to install - pip install chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split genres into list on comma and put each item on separate line\n",
    "genres = data_set['listed_in'].dropna().str.split(', ').explode().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use word cloud to visualize the most frequent genres in the Netflix library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word cloud using frequency of genres.\n",
    "plt.subplots(figsize=(10,10))\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='Black',\n",
    "                          width=1920,\n",
    "                          height=1080\n",
    "                         ).generate_from_frequencies(genres.value_counts())\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = data_set\n",
    "order =  sorted(netflix_data.release_year.unique())[-15:-1]\n",
    "plt.figure(figsize=(15,7))\n",
    "g = sns.countplot(netflix_data.release_year, hue=netflix_data.type, order=order, palette=\"pastel\");\n",
    "plt.title(\"Movies vs TV-Shows released on Netflix\")\n",
    "plt.xlabel(\"Production year\")\n",
    "plt.ylabel(\"Total Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"rating\", data=netflix_data, palette=\"Set2\", order=netflix_data['rating'].value_counts().index[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_order =  ['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']\n",
    "\n",
    "movie_rating = netflix_data['rating'].value_counts()\n",
    "#tv_rating = tv_show['rating'].value_counts()[rating_order].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = temp2.explode('listed_in')\n",
    "temp3 = temp3.rename(columns={'listed_in': 'Genre', 'rating': 'PG-Rating'})\n",
    "df = temp3.groupby(['Genre','PG-Rating']).size().unstack(fill_value=0)\n",
    "df = df[df > 0].fillna(0)\n",
    "\n",
    "df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[1:]).set_title(\"Genre vs PG Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "        \"& Talk\": \"\",\n",
    "        \"Classic & Cult\": \"Classic, Cult\",\n",
    "        \"Features\": \"\",\n",
    "        \"Series\": \"\",\n",
    "        \"Comedy\": \"Comedies\",\n",
    "        \"British\": \"International\",\n",
    "        \"Spanish-Language\": \"International\",\n",
    "        \"Children & Family\": \"Kids'\",\n",
    "        \"TV Shows\": \"\",\n",
    "        \"Movies\": \"\",\n",
    "        \"Docuseries\": \"Documentaries\",\n",
    "        \"& Talk Shows\": \"\",\n",
    "        \"Stand-Up\": \"\",\n",
    "        \"TV\": \"\",\n",
    "        \"Shows\": \"\",\n",
    "        \" \": \"\",\n",
    "    }\n",
    "\n",
    "temp2 = netflix_data.copy()\n",
    "temp2.listed_in = temp2.listed_in.replace(replacements, regex=True).str.split(',').apply(lambda x: [i for i in x if i != 'International'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to plot the genres and total count of each genre. \n",
    "Separate into movies and tv shows.\n",
    "Decided to remove categories International Movies and International TV shows as these were overrepresented in the data set as they are given to all movies not from the US. This category is always coupled with another gerne and is therefore not seen as one of the main genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract movie genres\n",
    "genres_movies = data_set[data_set[\"type\"]==\"Movie\"]['listed_in'].dropna().str.split(', ').explode().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar plot of all movie genres\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y=genres_movies, order=genres_movies.value_counts(ascending=True).index[:-1]) #removed international movies\n",
    "plt.title(\"Movies by Genre\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.xlabel(\"Total Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract TV genres\n",
    "genres_tv = data_set[data_set[\"type\"]==\"TV Show\"]['listed_in'].dropna().str.split(', ').explode().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot TV genre count\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y=genres_tv, order=genres_tv.value_counts(ascending=True).index[:-1]) # Removed international TV shows\n",
    "plt.title(\"TV Shows by Genre\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.xlabel(\"Total Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "plt.pie(\n",
    "    [genre_value for genre_value in pop_movie_genre.values()],\n",
    "    labels=[genre_keys for genre_keys in pop_movie_genre.keys()],\n",
    "    autopct=None\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "plt.pie(\n",
    "    [genre_value for genre_value in pop_series_genre.values()],\n",
    "    labels=[genre_keys for genre_keys in pop_series_genre.keys()],\n",
    "    autopct=None\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the 5 most frequent movie and TV genres and plot with gear added to see if there are any patterns.\n",
    "International TV shows and Movies have again been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_time = data_set[['date_added','listed_in']].copy()\n",
    "genre_time = genre_time[genre_time['date_added'] != 'Unknown date_added']\n",
    "genre_time['month_added'] = genre_time['date_added'].str.replace(',', '').str.lstrip().apply(lambda x: dt.datetime.strptime(x,'%B %d %Y')).dt.month_name()\n",
    "genre_time['year_added'] = genre_time['date_added'].str.replace(',', '').str.lstrip().apply(lambda x: dt.datetime.strptime(x,'%B %d %Y')).dt.year\n",
    "#year_released = genre_time['date_added']\n",
    "genre_time['listed_in'] = genre_time['listed_in'].str.split(', ')\n",
    "genre_time = genre_time.explode('listed_in')\n",
    "#print(genre_time)\n",
    "\n",
    "filter_list_m = ['Dramas', 'Comedies', 'Documentaries', 'Action & Adventure', 'Independent Movies']\n",
    "filter_list_tv = [\"TV Dramas\", \"TV Comedies\", \"Crime TV Shows\", \"Kids' TV\", \"Docuseries\"]\n",
    "top_m_genres = genre_time[genre_time.listed_in.isin(filter_list_m)]\n",
    "top_tv_genres = genre_time[genre_time.listed_in.isin(filter_list_tv)]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "#fig.suptitle(\"Movies TV Shows added to Netflix by Year for top 5 genres\")\n",
    "fig.tight_layout()\n",
    "\n",
    "sns.countplot(ax=axes[0], x=\"year_added\", hue=\"listed_in\" ,data=top_m_genres, palette=\"pastel\")\n",
    "axes[0].set_title(\"Movies\")\n",
    "axes[0].set_xlabel(\"Year added\")\n",
    "axes[0].set_ylabel(\"Total count\")\n",
    "axes[0].legend(loc=2)\n",
    "\n",
    "sns.countplot(ax=axes[1], x=\"year_added\", hue=\"listed_in\" ,data=top_tv_genres, palette=\"pastel\")\n",
    "axes[1].set_title(\"TV Shows\")\n",
    "axes[1].set_xlabel(\"Year added\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot heatplot of genre and year added to Netflix to see if there are any patterns in what genres have been popular over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of from dataset with month added and genre\n",
    "year_genre = genre_time[[\"year_added\", \"listed_in\"]]\n",
    "\n",
    "# Group month added and genre and make table with value counts\n",
    "group_y = year_genre.groupby(\"listed_in\")\n",
    "group_y = group_y['year_added'].value_counts() #count values in month\n",
    "group_y = group_y.unstack() \n",
    "group_y = group_y.fillna(0) #fill nans with 0\n",
    "\n",
    "# Check table\n",
    "group_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(group_y, cmap=\"Greens\")\n",
    "plt.title(\"Heatmap of 15 most popular genres and year added to Netflix\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.xlabel(\"Month added to Netflix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat plot of month added to netflix for 15 most frequent genres. \n",
    "Want to check if there is a pattern in when in the year the genres are added to Netflix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of from dataset with year added and genre\n",
    "month_genre = genre_time[[\"month_added\", \"listed_in\"]]\n",
    "\n",
    "# Get most popular genres by value_counts() and only select these from data set\n",
    "popular_genres = data_set.listed_in.str.split(', ').explode().value_counts().index[:17]\n",
    "month_genre = month_genre[month_genre.listed_in.isin(popular_genres)]\n",
    "\n",
    "# Remove International TV shows and International Movies as these are categories that does not give the genre of the movie, only that is was not made in the US. There are overrepresented in the dataset and not that interesting.\n",
    "month_genre = month_genre[month_genre.listed_in != 'International TV Shows']\n",
    "month_genre = month_genre[month_genre.listed_in != 'International Movies']\n",
    "\n",
    "# Group month added and genre and make table with value counts\n",
    "group = month_genre.groupby(\"listed_in\")\n",
    "group = group['month_added'].value_counts() #count values in month\n",
    "group = group.unstack() \n",
    "group = group.fillna(0) #fill nans with 0\n",
    "\n",
    "# Reindex to sort months by calendar and not alphabetically\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "group = group.reindex(columns=months) #sort months according to calendar\n",
    "\n",
    "# Check table\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(group, cmap=\"Greens\")\n",
    "plt.title(\"Heatmap of 15 most popular genres and month added to Netflix\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.xlabel(\"Month added to Netflix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there has been a change in added international movies and tv shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list_int = [\"International TV Shows\", \"International Movies\"]\n",
    "top_int = genre_time[genre_time.listed_in.isin(filter_list_int)]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x=\"year_added\", hue=\"listed_in\" ,data=top_int, palette=\"pastel\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check most frequent cast in the most frequent movie genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_cast = data_set[['cast','listed_in']].copy()\n",
    "genre_cast = genre_cast[genre_cast['cast'] != 'Unknown cast']\n",
    "genre_cast['cast'] = genre_cast['cast'].str.split(', ')\n",
    "genre_cast = genre_cast.explode('cast')\n",
    "\n",
    "m_per_cast = genre_cast['cast'].value_counts()\n",
    "\n",
    "genre_cast['listed_in'] = genre_cast['listed_in'].str.split(', ')\n",
    "genre_cast = genre_cast.explode('listed_in')\n",
    "\n",
    "top_comedy_cast = genre_cast[genre_cast['listed_in'] == 'Comedies']['cast']\n",
    "\n",
    "top_action_cast = genre_cast[genre_cast['listed_in'] == 'Action & Adventure']['cast']\n",
    "\n",
    "top_thriller_cast = genre_cast[genre_cast['listed_in'] == 'Thrillers']['cast']\n",
    "top_drama_cast = genre_cast[genre_cast['listed_in'] == 'Dramas']['cast']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 8))\n",
    "#fig.suptitle(\"Movies TV Shows added to Netflix by Year for top 5 genres\")\n",
    "#fig.tight_layout()\n",
    "\n",
    "sns.countplot(ax=axes[0,0], y=top_comedy_cast, palette=\"pastel\", order=top_comedy_cast.value_counts(ascending=False).index[:10])\n",
    "axes[0,0].set_title(\"Comedies\")\n",
    "axes[0,0].set_xlabel(\"\")\n",
    "\n",
    "sns.countplot(ax=axes[0,1], y=top_action_cast, palette=\"pastel\", order=top_action_cast.value_counts(ascending=False).index[:10])\n",
    "axes[0,1].set_title(\"Action & Adventure\")\n",
    "axes[0,1].set_xlabel(\"\")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(ax=axes[1,0], y=top_drama_cast, palette=\"pastel\", order=top_drama_cast.value_counts(ascending=False).index[:10])\n",
    "axes[1,0].set_title(\"Dramas\")\n",
    "axes[1,0].set_xlabel(\"No. of Movies\")\n",
    "\n",
    "sns.countplot(ax=axes[1,1], y=top_thriller_cast, palette=\"pastel\", order=top_thriller_cast.value_counts(ascending=False).index[:10])\n",
    "axes[1,1].set_title(\"Thrillers\")\n",
    "axes[1,1].set_xlabel(\"No. of Movies\")\n",
    "axes[1,1].set_ylabel(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check most frequent directors in most frequent movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_director = data_set[['director','listed_in']].copy()\n",
    "genre_director = genre_director[genre_director['director'] != 'Unknown director']\n",
    "genre_director['director'] = genre_director['director'].str.split(',')\n",
    "genre_director = genre_director.explode('director')\n",
    "genre_director['director'] = genre_director['director'].str.strip()\n",
    "\n",
    "m_per_dir = genre_director['director'].value_counts()\n",
    "\n",
    "genre_director['listed_in'] = genre_director['listed_in'].str.split(',')\n",
    "genre_director = genre_director.explode('listed_in')\n",
    "genre_director['listed_in'] = genre_director['listed_in'].str.lstrip()\n",
    "\n",
    "\n",
    "top_comedy_dir = genre_director[genre_director['listed_in'] == 'Stand-Up Comedy']['director']\n",
    "\n",
    "top_action_dir = genre_director[genre_director['listed_in'] == 'Action & Adventure']['director']\n",
    "\n",
    "top_thriller_dir = genre_director[genre_director['listed_in'] == 'Thrillers']['director']\n",
    "top_doc_dir = genre_director[genre_director['listed_in'] == 'Documentaries']['director']\n",
    "top_drama_dir = genre_director[genre_director['listed_in'] == 'Dramas']['director']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 8))\n",
    "#fig.suptitle(\"Movies TV Shows added to Netflix by Year for top 5 genres\")\n",
    "#fig.tight_layout()\n",
    "\n",
    "sns.countplot(ax=axes[0,0], y=top_comedy_dir, palette=\"pastel\", order=top_comedy_dir.value_counts(ascending=False).index[:10])\n",
    "axes[0,0].set_title(\"Stand-Up Comedy\")\n",
    "axes[0,0].set_xlabel(\"\")\n",
    "\n",
    "sns.countplot(ax=axes[0,1], y=top_action_dir, palette=\"pastel\", order=top_action_dir.value_counts(ascending=False).index[:10])\n",
    "axes[0,1].set_title(\"Action & Adventure\")\n",
    "axes[0,1].set_xlabel(\"\")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "\n",
    "sns.countplot(ax=axes[1,0], y=top_drama_dir, palette=\"pastel\", order=top_drama_dir.value_counts(ascending=False).index[:10])\n",
    "axes[1,0].set_title(\"Dramas\")\n",
    "axes[1,0].set_xlabel(\"No. of Movies\")\n",
    "\n",
    "sns.countplot(ax=axes[1,1], y=top_doc_dir, palette=\"pastel\", order=top_doc_dir.value_counts(ascending=False).index[:10])\n",
    "axes[1,1].set_title(\"Documentaries\")\n",
    "axes[1,1].set_xlabel(\"No. of Movies\")\n",
    "axes[1,1].set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying directors  from heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating director - genre matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populated = director_classification.populate_director_genre_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "# Copying dataframe in order to not directly mutating the populated list\n",
    "# as it takes some time to create the populatedDirector list\n",
    "copy = copy.copy(populated)\n",
    "\n",
    "for d in copy:\n",
    "    # Dropping directors columns that has less that 8 registered movies in total in addition to unknown director.\n",
    "    if (copy[d].sum() < 8.0 or d == 'Unknown director'):\n",
    "        copy.drop([d],  axis=1, inplace = True)\n",
    "\n",
    "sns.heatmap(copy) # Creating heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this heatmap containing the most active directors in the Netflix catalogue we can \"classify\" which genre a certain director is. By analysing the heatmap we can clearly state that \"Stand-Up Comedy\" is the genre that director is most active in and that Jan Suter is a \"Stand-Up Comedy\" director. McG can be with some certanty be classified as a \"Action & Adventure\" director."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres added per year (Sander)\n",
    "## The following plots shows the frequency of when genres were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import correlation_between_genres\n",
    "import matplotlib.pyplot as plt\n",
    "import chord\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv('netflix_titles.csv'))\n",
    "dfMovies = df[df['type'] == 'Movie']\n",
    "dfSeries = df[df['type'] == 'TV Show']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling null values\n",
    "### Number of movies and series which have missing date for when they were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingMovieDates = len(dfMovies[dfMovies['date_added'].isnull()])\n",
    "missingSeriesDates = len(dfSeries[dfSeries['date_added'].isnull()])\n",
    "print('Total number of movies having a unknown date they were added: ', missingMovieDates)\n",
    "print('Total number of series having a unknown date they were added: ', missingSeriesDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genresAddedPerYear(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Gets the dates from the dateFrame and converts the format to datetime\n",
    "    dates = pd.to_datetime(df['date_added'])\n",
    "    # Removes day and month, as we are only interested in the year\n",
    "    dates = dates.dt.year\n",
    "\n",
    "\n",
    "    # Splits the listed_in column into individual genre columns\n",
    "    genres = correlation_between_genres.genresOfMoviesSeries(df)\n",
    "\n",
    "    # Puts dates and genres into one table\n",
    "    genreAdded = correlation_between_genres.genresOfMoviesSeries(df)\n",
    "    genreAdded.insert(0, 'date_added', dates)\n",
    "\n",
    "    # Change cells with no value to None\n",
    "    genreAdded = genreAdded.where(genreAdded.notnull(), None)\n",
    "\n",
    "    # Name of every genre\n",
    "    uniqueGenres = correlation_between_genres.totalOccurenceOfGenres(genres).keys().tolist()\n",
    "    uniqueYears = genreAdded['date_added'].unique()\n",
    "    # Flip the list to get the columns in the next step in ascending order from left to right\n",
    "    uniqueYears = np.flip(uniqueYears)\n",
    "    # Removing nan value\n",
    "    uniqueYears = uniqueYears[1:]\n",
    "    uniqueYears = np.sort(uniqueYears)\n",
    "\n",
    "    genresAddedPerYear = pd.DataFrame(0, index = uniqueGenres, columns=uniqueYears)\n",
    "\n",
    "\n",
    "    for i, movie in genreAdded.iterrows():\n",
    "        yearAdded = movie['date_added']\n",
    "        genresOfMovie = movie[1:4]\n",
    "\n",
    "        for genre in genresOfMovie:\n",
    "            if (yearAdded == None or genre == None):\n",
    "                continue\n",
    "            genresAddedPerYear[yearAdded][genre] += 1\n",
    "    \n",
    "    return genresAddedPerYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame showing how many movies/series with a given genre was added per year. Mind that a movie/series may have multiple genres and the sum of each column isn't the same as at number of movies/series added per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genresAddedPerYearSeries = genresAddedPerYear(dfSeries)\n",
    "genresAddedPerYearMovies = genresAddedPerYear(dfMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the data in the year 2020 as the year is not over yet and therefore is not representable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genresAddedPerYearSeries = genresAddedPerYearSeries.drop(2020, axis = 'columns')\n",
    "genresAddedPerYearMovies = genresAddedPerYearMovies.drop(2020, axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(genresAddedPerYearSeries)\n",
    "display(genresAddedPerYearMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieGenres = genresAddedPerYearMovies.index\n",
    "seriesGenres = genresAddedPerYearSeries.index\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, )\n",
    "fig.subplots_adjust(right = 2, top = 1)\n",
    "\n",
    "ax1.set_xlabel('Years')\n",
    "ax1.set_ylabel('Added to genre')\n",
    "ax1.set_title('Movies added to genre per year', fontweight = 'bold')\n",
    "\n",
    "ax2.set_xlabel('Years')\n",
    "ax2.set_ylabel('Added to genre')\n",
    "ax2.set_title('Series added to genre per year', fontweight = 'bold')\n",
    "\n",
    "\n",
    "\n",
    "for genre in movieGenres:\n",
    "    ax1.plot(genresAddedPerYearMovies.loc[genre], marker = '.')\n",
    "\n",
    "for genre in seriesGenres:\n",
    "    ax2.plot(genresAddedPerYearSeries.loc[genre], marker = '.')\n",
    "\n",
    "ax1.legend(movieGenres, loc = 2, fontsize = 5)\n",
    "ax2.legend(seriesGenres, loc = 2, fontsize = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from these two plots that the amount of new content added has been increasing every year, and the growth started to spike around 2015. We can also tell that there hasn't been added any new series in the years between 2008 and 2012. There are in total 11 movies/series missing a added date where 10 of them are series. Some of these, or all, might have been added in the years between 2008 and 2012, but we don't really know."
   ]
  },
  {
   "source": [
    "## Combinations of genres"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genreCombos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Genre of every movie/series\n",
    "    genres = correlation_between_genres.genresOfMoviesSeries(df)\n",
    "\n",
    "    # How many occurences a genre needs to have to be included in the data\n",
    "    TOTAL_OCCURENCE_THRESHOLD = 100\n",
    "\n",
    "    # How many times a genre occurs\n",
    "    genreOccurence = correlation_between_genres.totalOccurenceOfGenres(genres)\n",
    "\n",
    "    # Only keeping genres that has an occurence higher than the threshold\n",
    "    genreOccurence = genreOccurence[genreOccurence > TOTAL_OCCURENCE_THRESHOLD]\n",
    "\n",
    "    # Cross-section between genres\n",
    "    corrMatrix = pd.DataFrame(index = genreOccurence.keys(), columns = genreOccurence.keys())\n",
    "\n",
    "    # Filling corrMatrix with values\n",
    "    for genre1 in corrMatrix.keys():\n",
    "        for genre2 in corrMatrix.keys():\n",
    "            mainGenre = correlation_between_genres.moviesSeriesWithGenre(genres, genre1)\n",
    "            genreCombination = correlation_between_genres.moviesSeriesWithGenre(mainGenre, genre2)\n",
    "\n",
    "            if(genre1 == genre2):\n",
    "                corrMatrix[genre1][genre2] = 0\n",
    "            else:\n",
    "                corrMatrix[genre1][genre2] = len(genreCombination)\n",
    "                \n",
    "    return corrMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of genres in movies\n",
    "NB! To show this plot open in JupyterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesGenreCombos = genreCombos(dfMovies)\n",
    "\n",
    "# Converts the cross-section matrix to a list as it is needed it the next step.\n",
    "genreCombinationValues = moviesGenreCombos.values.tolist()\n",
    "genreNames = moviesGenreCombos.index.tolist()\n",
    "\n",
    "# NOTE: Requires to be run in jupyter lab as the plot won't show in notebook.\n",
    "chord.Chord(genreCombinationValues, genreNames, margin=80, font_size_large='10px').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of genres in series\n",
    "NB! To show this plot open in JupyterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesGenreCombos = genreCombos(dfSeries)\n",
    "\n",
    "# Converts the cross-section matrix to a list as it is needed it the next step.\n",
    "genreCombinationValues = moviesGenreCombos.values.tolist()\n",
    "genreNames = moviesGenreCombos.index.tolist()\n",
    "\n",
    "# NOTE: Requires to be run in jupyter lab as the plot won't show in notebook.\n",
    "chord.Chord(genreCombinationValues, genreNames, margin=80, font_size_large='10px').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1.2 - Patterns in genres (Aleksander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work with desired data\n",
    "df = data_set[[\"type\",\"date_added\"]].copy()\n",
    "df = yrs_months.valid_dates(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get month and year columns\n",
    "df = yrs_months.create_month_column(df)\n",
    "df = yrs_months.create_year_column(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = yrs_months.all_months()\n",
    "years = yrs_months.all_years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yrs_months.create_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_months.heatmap(df,title=\"Content update per month and year\",xlab=\"Month\",ylab=\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Christmas\n",
    "df_xmas = data_set[[\"date_added\",\"description\"]].copy()\n",
    "df_xmas = df_xmas[df_xmas[\"description\"].str.contains(\"Christmas\")]\n",
    "df_xmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xmas = yrs_months.valid_dates(df_xmas) #Remova all dates with \"Uknown date_added\"\n",
    "df_xmas = yrs_months.create_month_column(df_xmas) #Create month column\n",
    "df_xmas = yrs_months.create_year_column(df_xmas) #Create year column\n",
    "df_xmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xmas_tab = yrs_months.create_table(df_xmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xmas_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_months.heatmap(df_xmas_tab,title=\"Overview of when Christmas content is added\",xlab=\"Months\",ylab=\"Years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all Horror related categories and see if more horror stuff comes before Halloween (should come in september/october)\n",
    "df_horror = data_set[[\"date_added\",\"listed_in\"]].copy()\n",
    "df_horror = df_horror[df_horror[\"listed_in\"].str.contains(\"Horror\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horror = yrs_months.valid_dates(df_horror)\n",
    "df_horror = yrs_months.create_month_column(df_horror)\n",
    "df_horror = yrs_months.create_year_column(df_horror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horror_tab = yrs_months.create_table(df_horror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horror_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_months.heatmap(df_horror_tab,title=\"Overview of when Horror content is added\",xlab=\"Months\",ylab=\"Years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Love movies around valentines? should be added in jan/feb\n",
    "\n",
    "#Find all Romantic related categories and see if more horror stuff comes before Halloween (should come in september/october)\n",
    "df_romantic = data_set[[\"date_added\",\"listed_in\"]].copy()\n",
    "df_romantic = df_romantic[df_romantic[\"listed_in\"].str.contains(\"Romantic\")]\n",
    "df_romantic = yrs_months.valid_dates(df_romantic)\n",
    "df_romantic = yrs_months.create_month_column(df_romantic)\n",
    "df_romantic = yrs_months.create_year_column(df_romantic)\n",
    "df_romantic_tab = yrs_months.create_table(df_romantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs_months.heatmap(df_romantic_tab,title=\"sometitle\",xlab=\"fksd\",ylab=\"fonsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content evolution of Netflix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library used for plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (35,6))\n",
    "sns.countplot(x='release_year', data = data_set, hue='type') # Plot the release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See when series and movies where added to netflix catalouge\n",
    "df = data_set.copy()\n",
    "df['year_added'] = df['date_added'].str[-4:]\n",
    "df['year_added'].replace({\"dded\": \"unknown\"}, inplace = True)\n",
    "df.sort_values(by=['year_added'], inplace=True)\n",
    "\n",
    "plt.figure(figsize = (35,10))\n",
    "sns.countplot(x=df['year_added'], data = df, hue='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check when they added new and old content to the library\n",
    "\n",
    "data_set['release_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ceck when netflix started to add older content to the catalouge\n",
    "\n",
    "df['decade'] = \"\"   # Make new column in the dataset\n",
    "\n",
    "# Make conditions for the plot\n",
    "conditions = [\n",
    "    (df['release_year'] >= 1925) & (df['release_year'] <= 1945),\n",
    "    (df['release_year'] >= 1946) & (df['release_year'] <= 1965),\n",
    "    (df['release_year'] >= 1966) & (df['release_year'] <= 1985),\n",
    "    (df['release_year'] >= 1986) & (df['release_year'] <= 2007),\n",
    "    (df['release_year'] >= 2008) & (df['release_year'] <= 2020)\n",
    "]\n",
    "\n",
    "values = ['1925-1945', '1946-1965', '1966-1985', '1986-2007', '2008-2020'] # Which value to add to the condition interval\n",
    "\n",
    "df['year'] = np.select(conditions, values)   # Add values to decade\n",
    "\n",
    "df.drop(df.loc[df['year']=='2008-2020'].index, inplace=True)  # Drop content made after netflixc was released\n",
    "\n",
    "\n",
    "plt.figure(figsize = (35,20))\n",
    "sns.countplot(x=df['year_added'], data = df, hue='year')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part is to create content recommondations based on a given title. \n",
    "In order to create recommondations, one needs to identify similarities between the given title and others titles in the dataset. \n",
    "\n",
    "The similarity between titles will be found by comparing and finding similarities in the description, genre and cast columns. \n",
    "\n",
    "To create recommondations, it is crucial to quantify the similarities. For this porpuse, the cosine similarity will be utlizied. The cosine similarity measures similarity between two vectors by comparing the angle between two vectors and determining if they are pointing in the same direction \\[kilde: Data Mining: Concepts and Techniques, chap 2.5.7]\n",
    "\n",
    "kilde link https://www.sciencedirect.com/science/article/pii/B9780123814791000022\n",
    "\n",
    "To fully understand this concept before the developement of the recommendation system, a simple example is explained:\n",
    "\n",
    "Consider two texts:\n",
    "\n",
    "text1 = Hello Hello Goodbye\n",
    "\n",
    "text2 = Goodbye Hello Goodbye\n",
    "\n",
    "By identifying the words and their frequency in the two texts, the table below is acheived: \n",
    "\n",
    "|  | Hello | Goodbye |\n",
    "| --- | --- | --- |\n",
    "| textA | 2 | 1 |\n",
    "| textB | 1 | 2 |\n",
    "\n",
    "This can now be visualized as vectors: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "text1_occurences = [2,1]\n",
    "text2_occurences = [1,2]\n",
    "vectors = np.array([text1_occurences,text2_occurences])\n",
    "origin = np.array([[0, 0],[0,0]])\n",
    "plt.quiver(*origin,vectors[:,0],vectors[:,1],color=[\"r\",\"g\"],angles=\"xy\",scale_units=\"xy\",scale=1)\n",
    "plt.xlim(0,2.5)\n",
    "plt.ylim(0,2.5)\n",
    "plt.xlabel(\"Goodbye\")\n",
    "plt.ylabel(\"Hello\")\n",
    "plt.text(0.4,0.5,\"\\u03B8\",fontsize=17)\n",
    "plt.text(1.02,2.02,\"textA\",fontsize=12,color=\"g\")\n",
    "plt.text(2.02,1.02,\"textB\",fontsize=12,color=\"r\")\n",
    "plt.title(\"Vectorized representation of frequency of words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By letting $\\theta$ be the angle between the two vectors, the cosine similarity  is calculated by: \n",
    "\\begin{equation}\n",
    "\\cos \\theta = \\frac{A \\cdot B}{ |A| \\cdot |B|},\n",
    "\\end{equation}\n",
    "where $A$ and $B$ are vectors representing the occurences of words in textA and textB, and $|A|$ and $|B|$ are the length of the vectors.\n",
    "Recalling from calculus the relation between a cosine value for $\\theta$ and the angle $\\theta$ itself:\n",
    "\n",
    "|  Degrees | cos $\\theta$ |\n",
    "|  --- | --- |\n",
    "|  0 | 1 |\n",
    "|  30 | 0.866 |\n",
    "| 60 |  0.5 |\n",
    "| 90 |  0 |\n",
    "\n",
    "For this angle-identifying reason, the cosine similarity is used to identify similar content. The smaller the angle $\\theta$ is between two vectors, the more similar the conent is. \n",
    "\n",
    "As mentioned above, the description, genre and cast column will be used to identify similarities. We start by extracting the dataframe with the respective columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_set[[\"title\",\"description\",\"listed_in\",\"cast\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to identify cosine similarities, we need to convert the information in the columns to strings. Therefore, the columns are modified and added to a seperate column. We start by removing stopwords from the description column and cleaning it up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the description column\n",
    "df[\"description\"] = df[\"description\"].str.lower() #converting all words to lower \n",
    "df[\"description\"] = df[\"description\"].str.split() #creating each description to a list\n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: pd.Series(''.join([word + ' ' for word in x if word not in stop_words]))) #need to get aslak to explain\n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: x.replace(',','')) #Removing commas\n",
    "df[\"description\"] = df[\"description\"].apply(lambda x: x.replace('.','')) #Removing dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the listed in column\n",
    "df[\"listed_in\"] = df[\"listed_in\"].str.lower()\n",
    "df[\"listed_in\"] = df[\"listed_in\"].str.split()\n",
    "df[\"listed_in\"] = df[\"listed_in\"].apply(lambda x: pd.Series(''.join([word + ' ' for word in x if word not in stop_words])))\n",
    "df[\"listed_in\"] = df[\"listed_in\"].apply(lambda x: x.replace(',',''))\n",
    "df[\"listed_in\"] = df[\"listed_in\"].apply(lambda x: x.replace('.',''))\n",
    "df[\"listed_in\"] = df[\"listed_in\"].apply(lambda x: x.replace('&',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the cast column\n",
    "df[\"cast\"] = df[\"cast\"].str.lower()\n",
    "df[\"cast\"] = df[\"cast\"].apply(lambda x: x.replace(' ','')) #removing whitespace between first and last name so cosine similraity checks entire actor name, and not only first name and last name\n",
    "df[\"cast\"] = df[\"cast\"].apply(lambda x: x.replace(',',' ')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine description, listed_in and cast as new column \n",
    "df[\"all_info\"] = df[\"description\"] + df[\"listed_in\"] + df[\"cast\"]\n",
    "#df.iloc[0].all_info run this line to see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we for each title have a single column containing information about the description, the genres and the cast, we can count instances of different words and create a matrix containing instances count of unique words. This can be acheived by using sklearn.feature_extraction.text.CountVectorizer, which is described to \"convert a collection of text documents to a matrix of token counts \\[source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() #Initialize a CountVectorizer object\n",
    "count_matrix = cv.fit_transform(df[\"all_info\"]) # Identify and count instances in the \"all_info\" column. \n",
    "# print(count_matrix.toarray()) print this line to get a feeling of what the count_matrix looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the count_matrix is acheived, the next step is to find the cosine similarity. For this purpose, sklearn.metrics.pairwise.cosine_similarity is used \\[source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim = cosine_similarity(count_matrix) #Now the cosine similarities have been quantified\n",
    "#print(cos_sim) print this line to get a feeling of what cosine sims looks like\n",
    "\n",
    "#Defining two helper functions\n",
    "def get_title_from_idx(df,idx):\n",
    "    \"\"\"\n",
    "    Returns the title of a entry from a given index\n",
    "    \"\"\"\n",
    "    return df[df.index == idx].title.values[0]\n",
    "#df[df.index == index][\"title\"].values[0]\n",
    "\n",
    "def get_idx_from_title(df,title):\n",
    "    \"\"\"\n",
    "    Returns the index of a movie from a given title\n",
    "    \"\"\"\n",
    "    return df[df[\"title\"]==title].index.values.astype(int)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_user_likes = \"Transformers Prime\" #Test code for transformers prime\n",
    "title_idx = get_idx_from_title(df,title_user_likes) #identify the index of the title that the user likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the index of the movie, we can find the list of the cosine similarities for the specific title. This is done by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_cos_sim = cos_sim[title_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep track of the indexes elements in the specific_cos_sim scores. By enumerating, the array of cosine similarity scores are converted into tuples containing the index and the scores. Finally, by converting these tuples into a list, we get a list of tuples containing the index and the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_content = list(enumerate(specific_cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to sort the list of tuples based on similarity scores. \n",
    "\n",
    "This is acheived by using the built-in sorted() function in python. The sorted() function buils a new list. In the official documentation for the sorted()-function, it says that the key should specify a function of one argument that is used to extract a comparison key for each element in the iterable. We wish to sort on second value for each tuple (wich is the cosine similirity score). I therefore define a function to acheive this:\n",
    "\n",
    "source:\n",
    "https://docs.python.org/3/library/functions.html#sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_func(val):\n",
    "    return val[1]\n",
    "\n",
    "sorted_similar = sorted(similar_content,key=key_func,reverse=True) #using reverse because we wish to sort by descending order\n",
    "#note, could also have used key = lambda x: x[1]\n",
    "sorted_similar_suggestions = sorted_similar[1:] #Skip the first because this will suggest itself\n",
    "\n",
    "\n",
    "#Print top 5 similar content:\n",
    "i = 0\n",
    "print(\"Top 5 titles similar to \"+ title_user_likes + \" are:\\n\")\n",
    "for e in sorted_similar_suggestions:\n",
    "    print(get_title_from_idx(df,e[0])) #As above, we have a list of tuples, where index 0 is key and inde 1 is cosine sim score. Therefore give index (e[0]) to function.\n",
    "    i+=1\n",
    "    if i >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the above list, when entering Transformers Prime as liked content, the code returns content similar to this. This is specially noticeable as the top similar content is another Transformers movie, and we can therefore conclude that this recommondation system works fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Ex 4. Try to predict the genre of a movie/show based on the director, actors, etc. using\n",
    "machine/deep learning techniques.\n",
    "\n",
    "Here we take a dive into a Neural Network model. It was decided to use the model [MLPClassifier][MLPClassifier] from the sklearn library. We will start this chapter with some displamers. \n",
    "\n",
    "Only the finalized model will be displayed here. The data has been polished the python file _cleaning_data_for_NN.py_ to improve the final product. The changes has been mainly to combine similar genres and to remove names of actor/actresses that were only displayed once in the dataset. We believed this would improve our model. \n",
    "\n",
    "As stated, the model has been ran several times, however, only the final model has been displayed below.Furthermore, the previous model has been saved and their report is located below in the [result](#Results) section.\n",
    "\n",
    "\n",
    "## Programming Logic\n",
    "\n",
    "The core of the programming logic for the final model is simple. In the _cleaning_data_for_NN.py_ file we start by cleaning the provided [netflix data][netflixData] set. Removing nan values and values that were incoherent. Movies/TV Shows labled _just_ Movies/TV Shows were dropped from the dataset, and genres labeled with \"TV Show\" or \"Movie\", as for example Romantic Movies ( or Romantic TV Shows) were renamed to just Romantic. The main reason were try to improve the amount of datapoints with the same genre and reducing the amount of classes in the output data. \n",
    "\n",
    "Afterwards the data were checked for duplicates. We had issues with some lables because they were mentioned twice in a row. The biggest issue were about \"International Movies/TV Shows\". Our decision to combine all International subject to a larger genre called \"International\" made it often appeared more than once in several row it was a subject of. We decided to remove duplicates and the Pandas Dataframe were saved to another CSV file, called _cleaned_Netflix_for_NN.csv_.\n",
    "\n",
    "[MLPClassifier]: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "[netflixData]: https://www.kaggle.com/shivamb/netflix-shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading of CSV file, creating and transforming X & y data.\n",
    "\n",
    "The file _final_NN_script.py_ contains the logic of the final Neural Network algorithm. It will be presented in parts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data=pd.read_csv(\"cleaned_Netflix_for_NN.csv\")\n",
    "\n",
    "smaller_data = netflix_data.copy()\n",
    "\n",
    "y = smaller_data.listed_in\n",
    "X = [','.join((d, c, r, t)) for d,c,r,t in zip(\n",
    "                                                smaller_data.director, \n",
    "                                                smaller_data.cast, \n",
    "                                                smaller_data.rating, \n",
    "                                                smaller_data.title\n",
    "                                            )]\n",
    "\n",
    "# Custom stop words for the CountVectorizer to ignore while transforming.\n",
    "customStopWords=['no cast', 'no director', 'movies', 'tv shows',\n",
    "                'lgbtq movies', 'teen tv shows', 'cult'] \n",
    "\n",
    "# Find all actors that only appears once in the dataset\n",
    "customCastStopWords = smaller_data.cast.str.split(', ').explode().value_counts()[\n",
    "    smaller_data.cast.str.split(', ').explode().value_counts() < 2].keys()\n",
    "customCastStopWords = [x.lower() for x in customCastStopWords] # Make all values lowercase\n",
    "\n",
    "# Add stopwords for vectorizer into single frozenset\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(customStopWords)\n",
    "stop_words = stop_words.union(list(customCastStopWords))\n",
    "\n",
    "# Split data into train and test data, training data = 80% of original data & test = 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1000) \n",
    "\n",
    "# Define the vectorizer algorithm\n",
    "matrix = CountVectorizer(\n",
    "    tokenizer=lambda row: [x.strip() for x in row.split(',') if x != ''], \n",
    "    stop_words=stop_words)\n",
    "\n",
    "# Transform X data\n",
    "x_train_fit = matrix.fit_transform(X_train)\n",
    "x_test_fit = matrix.transform(X_test)\n",
    "\n",
    "# Transform y data\n",
    "y_train_fit = matrix.fit_transform(y_train)\n",
    "y_test_fit = matrix.transform(y_test)\n",
    "\n",
    "# Printing out all genres in the y data\n",
    "print(\"Genres:\")\n",
    "for i in matrix.get_feature_names():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X & y data\n",
    "\n",
    "A few lines needs to be explained in the code section above. The fist is the making of the Y and X data (out and in data) used by the Neural Network algorithm. \n",
    "```python\n",
    "y = smaller_data.listed_in\n",
    "X = [','.join((d, c, r, t)) for d,c,r,t in zip(\n",
    "                                                smaller_data.director, \n",
    "                                                smaller_data.cast, \n",
    "                                                smaller_data.rating, \n",
    "                                                smaller_data.title\n",
    "                                            )]\n",
    "```\n",
    "\n",
    "y data is the output prediction, and in this model we are trying to predict genres from numerous columns of the data set. Alittle selfexplanatory that the y (out) data points needs to be the genres, which, in the dataset is called _listed_in_\n",
    "\n",
    "The X data has a simple ( and potentionally improved ) python _join_ method implementation. We make use of the _CountVectorizer_ method deployed by the sklearn library. This will be further explained later, now we just need to know the input of this can be either a string or byte. \n",
    "\n",
    "[From the countVectorizer documentation][countVectorizer]: \n",
    "> input : string {filename, file, content}, default=content <br>\n",
    "> _Otherwise the input is expected to be a sequence of items that can be of type string or byte._\n",
    "\n",
    "To get around this issue (easily) since we want several columns the join method were implemented. Here, for each row, we join all directors, cast members, the rating and the tile into single list of string. This is discussed in the [Improvment Chapter](#Improvments-and-further-research)\n",
    "\n",
    "[countVectorizer]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "#### Stopwords\n",
    "\n",
    "The next section explained is the stopword logic. Because the netflix data set has empty cells from nearly each column, we made use of the imputation method explained earlier and replaces the empty words with filler words as _No Director_ or _No Cast_. Because these words is selfmade and has no meaning, they needs to not be counted by the CountVectorizer method. Here the stopwords come into play. By writing _No Director_ or _No Cast_ as stopwords (among others), we can rely on the method to not take these words into account when making the fitted data.\n",
    "\n",
    "\n",
    "```python\n",
    "# Custom stop words for the CountVectorizer to ignore while transforming.\n",
    "customStopWords=['no cast', 'no director', 'movies', 'tv shows',\n",
    "                'lgbtq movies', 'teen tv shows', 'cult'] \n",
    "\n",
    "customCastStopWords = smaller_data.cast.str.split(', ').explode().value_counts()[\n",
    "    smaller_data.cast.str.split(', ').explode().value_counts() < 2].keys()\n",
    "                                                                                 \n",
    "customCastStopWords = [x.lower() for x in customCastStopWords] # Make all values lowercase\n",
    "\n",
    "# Add stopwords for vectorizer into single frozenset\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(customStopWords)\n",
    "stop_words = stop_words.union(list(customCastStopWords))\n",
    "```\n",
    "\n",
    "This section of code creates the stopwords for the final model. From previous test the ``` 'lgbtq movies', 'teen tv shows', 'cult' ``` values were added to the stopwords due to the results from the [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) from a previous model. \n",
    "Further it was decided to remove all actors that only appeared in one movie. The variable _customCastStopWords_ stores these values. \n",
    "\n",
    "#### CountVectorizer\n",
    "\n",
    "After splitting the data into both test and train datasets with a ratio of 20 to 80 percent of the original dataset, the CountVectorizer is used. The documentation explains the class from sklearn as this: \n",
    "> Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "The method is implemented with a lamdba function: ```  tokenizer=lambda row: [x.strip() for x in row.split(',') if x != ''] ``` The tokenizer paramter is explained as such: \n",
    "> tokenizer : callable, default=None <br>\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. \n",
    "\n",
    "This is the parameter that is used to split the words from each row. The lamdba function has been implemented to make use of the join method used to join all the different columns. Now each row gets split and stripped for whitespaces to make sure that every item is the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 1000 #round(x_train_fit.shape[1]*(2/3) + y_train_fit.shape[1])\n",
    "\n",
    "# Small datetime to check when ML started\n",
    "datetime_object = datetime.datetime.now()\n",
    "print(\"Begin ML: \", datetime_object)\n",
    "\n",
    "# Neural Network algorithm\n",
    "clf = MLPClassifier(hidden_layer_sizes=(hidden_layer,hidden_layer ),\n",
    "                    solver='adam', verbose=True, \n",
    "                    random_state=1, max_iter=50) \n",
    "\n",
    "clf.fit(x_train_fit, y_train_fit)\n",
    "\n",
    "# Small datetime to check when ML stopped\n",
    "datetime_object = datetime.datetime.now()\n",
    "print(\"End ML: \", datetime_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As this defined as a unbalanced multilabeled classification, we can look at the micro average results to best see the results of the NN model.\n",
    "By having the _drama_ genre as an example: \n",
    "\n",
    "Precision is calculated: <br>\n",
    "drama correctly indentified divided by drama correctly identified plus other genres identified as drama\n",
    "\n",
    "Recall is calculated:\n",
    "Drama correctly identified by drama correctly identified plus drama identified as other genres. \n",
    "##### First model\n",
    "\n",
    "First we present the Classification Report of the First model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from first_NN_model import first_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = first_NN_model()\n",
    "\n",
    "fileName = 'first_unflitered_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cleaning up the data we can still see a good precision score, however the recall score is lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving genre data\n",
    "\n",
    "The first attempt to improve the results were to redefine genres. Previous report says several genres had a precision and recall score of 0, thus we made the decision to combine the genres and/or removed them from the transformed dataset. The resulting report is seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaned_genres_script import cleaned_genres_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = cleaned_genres_NN_model()\n",
    "\n",
    "fileName = 'cleaned_genres_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows a good improvment from 0.19 to 0.38 when we cleaned the genres up. We halved the amount of genres from 42  to 22. Our hypothesis to get more refined genres had a positiv result. However, we can still try to get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final result\n",
    "\n",
    "The final model removed all actors that only appeared in one movie, (approx 20k names). The logic is seen in the [first subchapter of Programming Logic](#Reading-of-CSV-file,-creating-and-transforming-X-&-y-data.) chapter. We assumed that these names would only confuse the model and not be helpful. The results are printed below in the already saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import values needed for classification report\n",
    "from final_NN_script import final_NN_model\n",
    "\n",
    "x_train_fit, x_test_fit, y_train_fit, y_test_fit, matrix = final_NN_model()\n",
    "# Compute results\n",
    "fileName = 'Final_NN_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(fileName, 'rb'))\n",
    "y_pred = loaded_model.predict(x_test_fit)\n",
    "\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test_fit,y_pred, target_names=list(matrix.get_feature_names())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually see an further improvment on the recall and a small reduction in the precision. Can further small adjustments improve the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = multilabel_confusion_matrix(y_test_fit, y_pred)\n",
    "\n",
    "# Make labels out of genres\n",
    "labels = matrix.get_feature_names()\n",
    " \n",
    "def mutlilabel_cm_plot(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_xlabel('True label')\n",
    "    axes.set_ylabel('Predicted label')\n",
    "    axes.set_title(class_label)\n",
    "    \n",
    "# Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(6, 4, figsize=(7, 12))\n",
    "    \n",
    "for axes, cfs_matrix, label in zip(ax.flatten(), cm, labels):\n",
    "    mutlilabel_cm_plot(cfs_matrix, axes, label, [\"Y\", \"N\"])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end we show the confusion matrixes of all the genres from the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvments and further research\n",
    "\n",
    "Here arqe some features that could have been improved given more knowledge and time\n",
    "\n",
    "#### Questions\n",
    "* Could a word embedding model be used to further generalize the names of actor/actresses? \n",
    "* Could the title be used in a better extend? Maybe split and remove more of the same word and not use whole title string as an node?\n",
    "* Could word embedding be used to generelize the training/test data better than the manuall cleaning?\n",
    "\n",
    "#### Improvments\n",
    "* The model could be more refined to accept functions and/or make it more resuable.\n",
    "* The cleaning could be refined to accept different datasets, the genre that were changed is now hardcoded in a dictonary. This is not optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_sanitizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataSet = pd.read_csv('../netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv('../netflix_titles.csv')\n",
    "# Copy listed_in column into it's own dataframe, split each genre into it's own columns and rename the columns\n",
    "genres = dataSet['listed_in'].str.split(', ', expand=True)\n",
    "genres.rename(columns = {0: 'genre1', 1: 'genre2', 2: 'genre3'}, inplace = True)\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above for cast and director columns\n",
    "cast = dataSet['cast'].str.split(', ', expand = True)\n",
    "director = dataSet['director'].str.split(', ', expand = True)\n",
    "\n",
    "for i in cast.keys():\n",
    "    cast.rename(columns={i: 'actor{}'.format(i + 1)}, inplace = True)\n",
    "\n",
    "for i in director.keys():\n",
    "    director.rename(columns={i: 'director{}'.format(i + 1)}, inplace = True)\n",
    "\n",
    "cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove International Movies and International TV Shows from genres as it is overrepresented and doesn't really tell us much\n",
    "\n",
    "genres = genres.replace({'International Movies': None})\n",
    "genres = genres.replace({'International TV Shows': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set any cell that doesn't have a value to nan\n",
    "director = director[director.notnull()]\n",
    "cast = cast[cast.notnull()]\n",
    "genres = genres[genres.notnull()]\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch how many times each director occurs in movies and series\n",
    "directorOccurence = pd.Series()\n",
    "\n",
    "for col in director:\n",
    "    count = director[col].value_counts()\n",
    "    directorOccurence = directorOccurence.add(count, fill_value = 0)\n",
    "\n",
    "directorOccurence = directorOccurence.sort_values(ascending=False)\n",
    "\n",
    "# Which directors occur in more than 7 movies and series\n",
    "directorOccurence[directorOccurence > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for actors instead\n",
    "actorOccurence = pd.Series()\n",
    "\n",
    "for col in cast:\n",
    "    count = cast[col].value_counts()\n",
    "    actorOccurence = actorOccurence.add(count, fill_value = 0)\n",
    "\n",
    "actorOccurence = actorOccurence.sort_values(ascending=False)\n",
    "\n",
    "# Actors that occur in more than 2 movies and series\n",
    "actorOccurence\n",
    "actorOccurence[actorOccurence > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast.count()\n",
    "# We can tell from this that there are very few data entries in the columns further out and we choose to drop\n",
    "# these as there won't be much information lost compared to the potential gain in memory saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to keep columns with more than 100 data points\n",
    "\n",
    "# Potential problem with this approach is that we could lose informtion on the same actor in the case where an actor always occurs as one of the last everytime\n",
    "cast = cast[cast.count().keys()[cast.count().values > 100]]\n",
    "cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.subplots_adjust(right = 3, top = 2)\n",
    "\n",
    "# Plot comparing the directors who occurs the most.\n",
    "directorsTop = directorOccurence.iloc[0:50].sort_values()\n",
    "\n",
    "ax1.set_title('Top 50 directors by occurance', fontweight = 'bold')\n",
    "ax1.set_xlabel('Occurance')\n",
    "ax1.barh(width = directorsTop.values, y = directorsTop.keys())\n",
    "\n",
    "# Plot comparing the actors who occurs the most.\n",
    "actorsTop = actorOccurence.iloc[0:50].sort_values()\n",
    "\n",
    "ax2.set_title('Top 50 actors by occurance', fontweight = 'bold')\n",
    "ax2.set_xlabel('Occurance')\n",
    "ax2.barh(width = actorsTop.values, y = actorsTop.keys())\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any common genres the directors occur in.\n",
    "\n",
    "# Set to any value between 0 and 50 where 50 is the director with the highest occurance\n",
    "DIRECTOR_INDEX = 25\n",
    "\n",
    "director_with_highest_occurance = directorsTop.index[DIRECTOR_INDEX]\n",
    "\n",
    "movies_series_director_occurs_indexes = []\n",
    "\n",
    "for col in director.columns:\n",
    "    movies_series_director_occurs_indexes += director.loc[director[col] == director_with_highest_occurance].index.values.tolist()\n",
    "\n",
    "\n",
    "genres.iloc[movies_series_director_occurs_indexes]\n",
    "\n",
    "# We can tell from this that there actually seems to be a really big correlation between director and genres (at least for the top 25) as a director\n",
    "# keeps making movies/series in the same genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any common genres the actors occur in.\n",
    "\n",
    "# Set to any value between 0 and 50 where 50 is the actor with the highest occurance\n",
    "ACTOR_INDEX = 25\n",
    "\n",
    "actor_with_highest_occurance = actorsTop.index[ACTOR_INDEX]\n",
    "\n",
    "movies_series_actor_occurs_indexes = []\n",
    "\n",
    "\n",
    "\n",
    "for col in cast.columns:\n",
    "    movies_series_actor_occurs_indexes += cast.loc[cast[col] == actor_with_highest_occurance].index.values.tolist()\n",
    "\n",
    "\n",
    "genres.iloc[movies_series_actor_occurs_indexes]\n",
    "\n",
    "# There seems to be some correlation between an actor and genres as well, but not as strong as with the directors. As\n",
    "# many of the actors do occur in the same genres multiple times, but they also deviate from their \"main genre\" at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relationship between directors and genres, and actors and genres, can be used to predict the genre of a movie based on director and actor, but we got an issue that the number of movies/series an actor/director has been a part of drops of quickly as we can tell from the occurance plots. This means that if we would want to use this information to create a model using machine learning, it could become hard to get a model with high accuracy if what we input to the model doesn't contain any of the directors or actors that has a high occurance, and there isn't particulary many of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}